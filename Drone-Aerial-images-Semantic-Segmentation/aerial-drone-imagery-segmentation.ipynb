{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # Linear algebra\n",
    "from PIL import Image, ImageEnhance # Image processing\n",
    "import matplotlib.pyplot as plt # To plot batche images and its masks\n",
    "import torch # PyTorch\n",
    "from torch import nn # To add custom layers\n",
    "from torch import optim # Optimizer to update model parameters\n",
    "import torch.nn.functional as F # Functional requirements\n",
    "from torch.utils.data import Dataset, DataLoader # To make custom dataset class & to batch data\n",
    "from torchvision import models, transforms # Get pretrained models & to make data augmentation pipeline\n",
    "from torch.utils.data import Subset # To divide data into train, val & test sets\n",
    "import os # To iterate through dirs, file read-write stuff.\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "   A callable class to apply random cropping on a image and its mask.\n",
    "   \n",
    "   ...\n",
    "\n",
    "   Attributes\n",
    "   ----------\n",
    "   min_crop_width : int\n",
    "       Minimum width of a region to randomly crop from image.\n",
    "   min_crop_height : int\n",
    "       Minimum height of a region to randomly crop from image.\n",
    "\n",
    "   Methods\n",
    "   -------\n",
    "   __call__(sample):\n",
    "       Given tuple of input image and its corresponding mask image, randomly a region will be cropped from both image and mask.\n",
    "       Width of this region will be b.w. min_crop_width and image width, analogously for height.\n",
    "       This region will be extracted from input image and mask image.\n",
    "       This cropped image and cropped mask will be returned as a tuple\n",
    "       \n",
    "   \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, min_crop_width=224, min_crop_height=224):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing random cropping\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            min_crop_width : int\n",
    "                Minimum width of a region to randomly crop from image.\n",
    "            min_crop_height : int\n",
    "                Minimum height of a region to randomly crop from image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure min val of parameters are at least 224 pixels\n",
    "        assert (min_crop_width >= 224 and min_crop_height >= 224)\n",
    "\n",
    "        self.min_crop_width = min_crop_width\n",
    "        self.min_crop_height = min_crop_height\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns randomly cropped region from image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of randomly cropped image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        if np.random.uniform() > 0.5:\n",
    "            # Randomly choose width b.w. [self.min_crop_width, image width], analogously for height\n",
    "            desired_width = np.random.randint(low=self.min_crop_width, high=width + 1)\n",
    "            desired_height = np.random.randint(low=self.min_crop_height, high=height + 1)\n",
    "\n",
    "            # Randomly select top left point of our region\n",
    "            x = np.random.randint(low=0, high=width - desired_width + 1)\n",
    "            y = np.random.randint(low=0, high=height - desired_height + 1)\n",
    "\n",
    "            # Perform actual cropping on image and mask given above parameters\n",
    "            img = img.crop((x, y, x + desired_width - 1, y + desired_height - 1))\n",
    "            mask = mask.crop((x, y, x + desired_width - 1, y + desired_height - 1))\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"\n",
    "    A callable class to resize input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    desired_width : int\n",
    "        Desired width to resize the image to.\n",
    "    desired_height : int\n",
    "        Desired height to resize the image to.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Width and height for resize operation will be provided while object init.\n",
    "        Resized image and its mask will be returned as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, desired_width=1024, desired_height=1024):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing resizing operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            desired_width : int\n",
    "                Desired width to resize the image to.\n",
    "            desired_height : int\n",
    "                Desired height to resize the image to.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure parameter values are atlease 224 pixels\n",
    "        assert (desired_width >= 224 and desired_height >= 224)\n",
    "\n",
    "        self.desired_width = desired_width\n",
    "        self.desired_height = desired_height\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns resized image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of resized image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        # Performs actual resizing of image and mask\n",
    "        img = img.resize((self.desired_width, self.desired_height), resample=Image.NEAREST)\n",
    "        mask = mask.resize((self.desired_width, self.desired_height), resample=Image.NEAREST)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class ColorTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to apply color transform on just the input image, not mask image.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    brightness : float\n",
    "        Factor required to change the brightness of the image\n",
    "    contrast : float\n",
    "        Factor required to change the contrast of the image\n",
    "    saturation : float\n",
    "        Factor required to change the saturation of the image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        A number b.w. [max(0, 1 - self.brightness), 1 + self.brightness] generated for all three params.\n",
    "        Apply color transform on input image based on above generated parameters.\n",
    "        Returns transformed input image and its unchanged mask as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, brightness=0., contrast=0., saturation=0.):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing color transform operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            brightness : float\n",
    "                Factor required to change the brightness of the image\n",
    "            contrast : float\n",
    "                Factor required to change the contrast of the image\n",
    "            saturation : float\n",
    "                Factor required to change the saturation of the image\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure param values aren't negative\n",
    "        assert (brightness >= 0 and contrast >= 0 and saturation >= 0)\n",
    "\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns color transformed input image and its unchanged mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of color transformed input image and its unchanged mask.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "\n",
    "        # Generate a number b.w. [max(0, 1 - self.brightness), 1 + self.brightness] likewise for other two params\n",
    "        brightness_factor = np.random.uniform(max(0, 1 - self.brightness), 1 + self.brightness)\n",
    "        contrast_factor = np.random.uniform(max(0, 1 - self.contrast), 1 + self.contrast)\n",
    "        saturation_factor = np.random.uniform(max(0, 1 - self.saturation), 1 + self.saturation)\n",
    "\n",
    "        # For factor < 1 brightness of the image will be decreased, analogously for contrast and saturation.\n",
    "        # For factor = 1 original brightness will be retained, analogously for contrast and saturation.\n",
    "        # For factor > 1 brightness of the image will be increased, analogously for contrast and saturation.\n",
    "\n",
    "        # Apply actual color transform on input image based on above generated params.\n",
    "        img = ImageEnhance.Brightness(img).enhance(brightness_factor)\n",
    "        img = ImageEnhance.Contrast(img).enhance(contrast_factor)\n",
    "        img = ImageEnhance.Color(img).enhance(saturation_factor)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class FlipTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to flip input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Randomly flip (horizontally and vertically) both input image and mask image.\n",
    "        Resized image and its mask will be returned as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns resized image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of resized image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get image and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "\n",
    "        # If randomly generated number (b.w. 0 and 1) is > 0.5 then flip (input image and mask image) horizontally else not.\n",
    "        if np.random.uniform() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # If randomly generated number (b.w. 0 and 1) is > 0.5 then flip (input image and mask image) vertically else not.\n",
    "        if np.random.uniform() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class AffineTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to apply affine transform on input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    degrees : float\n",
    "        Required to rotate image and mask to some random angle\n",
    "    translate : tuple of 2 vals\n",
    "        Required to shift (by dx in x-direction and by dy in y-direction) image and mask to some random parameters.\n",
    "    scale : tuple of 2 vals\n",
    "        Required to scale image and mask to some random param (like downscale or upscale to some degree).\n",
    "    shear : tuple of 4 vals\n",
    "        Required to shear image and mask to some random params.\n",
    "    resample : int\n",
    "        Resampling method for affine transform. Default is nearest neighbor strategy for segmentation task.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Draw random values given init args for rotation, translation, scaling and shearing.\n",
    "        Returns affine transformed image and its mask as a tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees=0., translate=(0, 0), scale=(1., 1.), shear=(0., 0., 0., 0.), resample=Image.BILINEAR):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing affine transform.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            degrees : float\n",
    "                Required to rotate image and mask to some random angle\n",
    "            translate : tuple of 2 vals\n",
    "                Required to shift (by dx in x-direction and by dy in y-direction) image and mask to some random parameters.\n",
    "            scale : tuple of 2 vals\n",
    "                Required to scale image and mask to some random param (like downscale or upscale to some degree).\n",
    "            shear : tuple of 4 vals\n",
    "                Required to shear image and mask to some random params.\n",
    "            resample : int\n",
    "                Resampling method for affine transform. Default is nearest neighbor strategy for segmentation task.\n",
    "        \"\"\"\n",
    "\n",
    "        assert (0 <= degrees <= 180)\n",
    "        assert (all(0 <= val <= 1 for val in translate))\n",
    "        assert (all(val > 0 for val in scale))\n",
    "        self.degrees = degrees\n",
    "        self.translate = translate\n",
    "        self.scale = scale\n",
    "        self.shear = shear\n",
    "        self.resample = resample\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns affine tranformed image and its mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of affine tranformed image and its mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get image and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        # Randomly generate angle b.w. [-self.degrees, self.degrees]\n",
    "        angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "\n",
    "        # Randomly generate dx and dy based on params to translate the image\n",
    "        max_dx = self.translate[0] * width\n",
    "        max_dy = self.translate[1] * height\n",
    "        translations = (np.round(np.random.uniform(-max_dx, max_dx)),\n",
    "                        np.round(np.random.uniform(-max_dy, max_dy)))\n",
    "\n",
    "        # Randomly generate new scale based on params\n",
    "        new_scale = np.random.uniform(self.scale[0], self.scale[1])\n",
    "\n",
    "        # Randomly generate shear ranges based on params\n",
    "        shear_ranges = [np.random.uniform(self.shear[0], self.shear[1]),\n",
    "                        np.random.uniform(self.shear[2], self.shear[3])]\n",
    "\n",
    "        # Apply affine transform based on above generated values on image and mask\n",
    "        img = transforms.functional.affine(img, angle=angle, translate=translations,\n",
    "                                           scale=new_scale, shear=shear_ranges, resample=self.resample)\n",
    "        mask = transforms.functional.affine(mask, angle=angle, translate=translations,\n",
    "                                            scale=new_scale, shear=shear_ranges, resample=self.resample)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Data augmentation pipeline for train images\n",
    "train_transform = transforms.Compose([RandomCrop(720, 720),\n",
    "                                      Resize(512, 512),\n",
    "                                      ColorTransform(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                                      FlipTransform(),\n",
    "                                      AffineTransform(degrees=180, translate=(.2, .2), scale=(0.75, 1.25),\n",
    "                                                      shear=[-30, 30, -30, 30], resample=Image.NEAREST)])\n",
    "\n",
    "# Transform for validation images\n",
    "val_transform = transforms.Compose([Resize(512, 512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, nb_classes = 20):\n",
    "        # Path containing input images\n",
    "        self.img_path = img_path\n",
    "        \n",
    "        # Path containing mask images\n",
    "        self.mask_path = mask_path\n",
    "        \n",
    "        # Number of classes in output segmentation map\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "        # Store filenames of all the images in a list\n",
    "        self.filenames = []\n",
    "        self.data_len = 0\n",
    "        for fn in os.listdir(img_path):\n",
    "            self.filenames.append(fn.split('.')[0])\n",
    "            self.data_len += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Returns dataset size\n",
    "        '''\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "         # Read image (mask image too) on given idx from list\n",
    "        img = Image.open(os.path.join(img_path, self.filenames[idx]) + '.jpg')\n",
    "        mask = Image.open(os.path.join(mask_path, self.filenames[idx]) + '.png')\n",
    "        \n",
    "        return img, mask\n",
    "    \n",
    "# Map inputs to desired state by passing them through augmentation pipeline\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform = None, nb_classes = 20, mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]):\n",
    "        self.dataset = dataset\n",
    "        self.nb_classes = nb_classes\n",
    "        self.transform = transform\n",
    "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            img, mask = self.transform(self.dataset[idx])\n",
    "            \n",
    "        # Plot augmented image and its corresponding mask\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#         plt.imshow(mask)\n",
    "#         plt.show()\n",
    "\n",
    "        # Convert image to torch tensor\n",
    "        img = self.toTensor(img)\n",
    "        mask = torch.from_numpy(np.array(mask))\n",
    "        \n",
    "        # Mean normalization on image and one hot encode the mask image\n",
    "        img = self.normalize(img)\n",
    "        mask = F.one_hot(mask.to(torch.int64), num_classes = self.nb_classes).permute(2, 0, 1).to(torch.float32)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "img_path = '../input/semantic-drone-dataset/semantic_drone_dataset/original_images/'\n",
    "mask_path = '../input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/'\n",
    "nb_classes = 23\n",
    "\n",
    "# Creating dataset class\n",
    "data = CustomDataset(img_path, mask_path, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create random indices for train, val and test set\n",
    "val_split = .1\n",
    "\n",
    "dataset_size = len(data)\n",
    "np.random.seed(4)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split = int(np.floor(val_split * dataset_size))\n",
    "\n",
    "# Divide the data into train, val and test dataset given above indices\n",
    "train_dataset = TransformDataset(Subset(data, indices[(2 * split):]), train_transform, 23)\n",
    "val_dataset = TransformDataset(Subset(data, indices[split: (2 * split)]), val_transform, 23)\n",
    "test_dataset = TransformDataset(Subset(data, indices[:split]), val_transform, 23)\n",
    "\n",
    "# Create data loader for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 6, num_workers = 2, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 40, num_workers = 2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 40, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss criterion for image segmentation problem\n",
    "# Combination of dice loss and cross entropy loss is excellent choice\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Apply softmax to get prob. distribution for each pixel\n",
    "        inputs = F.softmax(inputs, 1)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "# Dice score used as an accuracy metric\n",
    "def calculateDiceScore(preds, targets, smooth = 1):\n",
    "    preds = F.softmax(preds, 1)       \n",
    "        \n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (preds * targets).sum()                            \n",
    "    return (2.*intersection + smooth)/(preds.sum() + targets.sum() + smooth)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cadafcb32b4b11a424605ba259cfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n"
     ]
    }
   ],
   "source": [
    "# Loading model from torchvision\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=False, num_classes=21, aux_loss=None)\n",
    "\n",
    "# Change last layer for our requirement (we have 23 classes total including background)\n",
    "model.classifier[4] = nn.Conv2d(256, 23, (1, 1), (1, 1))\n",
    "model.aux_classifier[4] = nn.Conv2d(256, 23, (1, 1), (1, 1))\n",
    "\n",
    "# Freeze some of the initial layers to finetune the model\n",
    "idx = 0\n",
    "for name, param in model.named_parameters():\n",
    "#     print(idx, name)\n",
    "    if idx < 174:\n",
    "        param.requires_grad_ = False\n",
    "    else:\n",
    "        break\n",
    "    idx += 1\n",
    "    \n",
    "# Defining device to cuda if GPU is avaliable else to cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining loss criterion\n",
    "criterion = DiceBCELoss()\n",
    "\n",
    "# Defining optimizer to update model params, Adam's a good default\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Learning rate scheduler to update lr when loss stops improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTrain Loss: 0.6578396409749985\tVal Loss: 0.7305787205696106\n",
      "Epoch 2:\tTrain Loss: 0.522078313678503\tVal Loss: 0.5577294826507568\n",
      "Epoch 3:\tTrain Loss: 0.49104147860780356\tVal Loss: 0.5284269452095032\n",
      "Epoch 4:\tTrain Loss: 0.47382133547216654\tVal Loss: 0.5598229169845581\n",
      "Epoch 5:\tTrain Loss: 0.4813829264603555\tVal Loss: 0.5377707481384277\n",
      "Epoch 6:\tTrain Loss: 0.4486735393293202\tVal Loss: 0.46749550104141235\n",
      "Epoch 7:\tTrain Loss: 0.4438299208879471\tVal Loss: 0.49861422181129456\n",
      "Epoch 8:\tTrain Loss: 0.45085419733077287\tVal Loss: 0.6386446952819824\n",
      "Epoch 9:\tTrain Loss: 0.49487469494342806\tVal Loss: 0.5419107675552368\n",
      "Epoch 10:\tTrain Loss: 0.4718948112800717\tVal Loss: 0.5419255495071411\n",
      "Epoch 11:\tTrain Loss: 0.44347126316279173\tVal Loss: 0.4642212986946106\n",
      "Epoch 12:\tTrain Loss: 0.4422900528647006\tVal Loss: 0.44391128420829773\n",
      "Epoch 13:\tTrain Loss: 0.41906638666987417\tVal Loss: 0.43258798122406006\n",
      "Epoch 14:\tTrain Loss: 0.4131083437241614\tVal Loss: 0.43362778425216675\n",
      "Epoch 15:\tTrain Loss: 0.4206000448204577\tVal Loss: 0.42115509510040283\n",
      "Epoch 16:\tTrain Loss: 0.4292329673655331\tVal Loss: 0.4207431674003601\n",
      "Epoch 17:\tTrain Loss: 0.4275915866717696\tVal Loss: 0.4215445816516876\n",
      "Epoch 18:\tTrain Loss: 0.40390558829531076\tVal Loss: 0.4051598906517029\n",
      "Epoch 19:\tTrain Loss: 0.4074430398643017\tVal Loss: 0.4134232997894287\n",
      "Epoch 20:\tTrain Loss: 0.4023499168455601\tVal Loss: 0.40323421359062195\n",
      "Epoch 21:\tTrain Loss: 0.40892129112035036\tVal Loss: 0.40401479601860046\n",
      "Epoch 22:\tTrain Loss: 0.4095573551952839\tVal Loss: 0.41636282205581665\n",
      "Epoch 23:\tTrain Loss: 0.38975236006081104\tVal Loss: 0.3998092710971832\n",
      "Epoch 24:\tTrain Loss: 0.38793958378955723\tVal Loss: 0.39454132318496704\n",
      "Epoch 25:\tTrain Loss: 0.38061919528990984\tVal Loss: 0.3969537615776062\n",
      "Epoch 26:\tTrain Loss: 0.4020721264183521\tVal Loss: 0.4114053249359131\n",
      "Epoch 27:\tTrain Loss: 0.40005622413009406\tVal Loss: 0.43501418828964233\n",
      "Epoch 28:\tTrain Loss: 0.391471610032022\tVal Loss: 0.429130494594574\n",
      "Epoch 29:\tTrain Loss: 0.38755820896476506\tVal Loss: 0.4000134766101837\n",
      "Epoch 30:\tTrain Loss: 0.37890232214704156\tVal Loss: 0.38354337215423584\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-83f3039fc92b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# A pass through train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Move batch data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path at which to store model and required configs\n",
    "model_save_path = 'aerial_imagery_seg_model.pt'\n",
    "\n",
    "# Total epochs to train for\n",
    "epochs = 50\n",
    "\n",
    "# Keep track of min val loss\n",
    "min_val_loss = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0.\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # A pass through train dataset\n",
    "    for imgs, masks in train_loader:\n",
    "        # Move batch data to device\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass the batch and get predictions\n",
    "        preds = model(imgs)['out']\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(preds, masks)\n",
    "        \n",
    "        # Add to calculate loss for whole dataset\n",
    "        train_loss += (loss.item() * imgs.size(0))\n",
    "        \n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make weight updates\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Empty cuda cache to clear useless data from VRAM for better utilization\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Switch model to inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Check model performance on val set\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            preds = model(imgs)['out']\n",
    "\n",
    "            loss = criterion(preds, masks)\n",
    "            val_loss += (loss.item() * imgs.size(0))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    # Calculate avg train and avg val loss\n",
    "    train_loss /= len(train_dataset)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    # Decrease lr depending on val_loss & given params (object init)\n",
    "    scheduler.step(val_loss)\n",
    "        \n",
    "    # If loss is decreasing then store model in file else not\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "    print('Epoch {}:\\tTrain Loss: {}\\tVal Loss: {}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model from saved file\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Switch model to inference mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dice score on test set, to get test accuracy\n",
    "with torch.no_grad():\n",
    "    test_acc = 0       \n",
    "    for imgs, masks in test_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        preds = model(imgs)['out']\n",
    "\n",
    "        dice_score = calculateDiceScore(preds, masks).item()\n",
    "        test_acc += (dice_score * imgs.size(0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Test Acc: {}'.format(test_acc / len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
