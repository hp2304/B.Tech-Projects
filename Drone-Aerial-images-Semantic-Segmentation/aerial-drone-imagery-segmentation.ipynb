{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # Linear algebra\n",
    "from PIL import Image, ImageEnhance # Image processing\n",
    "import matplotlib.pyplot as plt # To plot batche images and its masks\n",
    "import torch # PyTorch\n",
    "from torch import nn # To add custom layers\n",
    "from torch import optim # Optimizer to update model parameters\n",
    "import torch.nn.functional as F # Functional requirements\n",
    "from torch.utils.data import Dataset, DataLoader # To make custom dataset class & to batch data\n",
    "from torchvision import models, transforms # Get pretrained models & to make data augmentation pipeline\n",
    "from torch.utils.data import Subset # To divide data into train, val & test sets\n",
    "import os # To iterate through dirs, file read-write stuff.\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../input/semantic-drone-dataset/semantic_drone_dataset/original_images/'\n",
    "mask_path = '../input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "   A callable class to apply random cropping on a image and its mask.\n",
    "   \n",
    "   ...\n",
    "\n",
    "   Attributes\n",
    "   ----------\n",
    "   min_crop_width : int\n",
    "       Minimum width of a region to randomly crop from image.\n",
    "   min_crop_height : int\n",
    "       Minimum height of a region to randomly crop from image.\n",
    "\n",
    "   Methods\n",
    "   -------\n",
    "   __call__(sample):\n",
    "       Given tuple of input image and its corresponding mask image, randomly a region will be cropped from both image and mask.\n",
    "       Width of this region will be b.w. min_crop_width and image width, analogously for height.\n",
    "       This region will be extracted from input image and mask image.\n",
    "       This cropped image and cropped mask will be returned as a tuple\n",
    "       \n",
    "   \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, min_crop_width=224, min_crop_height=224):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing random cropping\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            min_crop_width : int\n",
    "                Minimum width of a region to randomly crop from image.\n",
    "            min_crop_height : int\n",
    "                Minimum height of a region to randomly crop from image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure min val of parameters are at least 224 pixels\n",
    "        assert (min_crop_width >= 224 and min_crop_height >= 224)\n",
    "\n",
    "        self.min_crop_width = min_crop_width\n",
    "        self.min_crop_height = min_crop_height\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns randomly cropped region from image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of randomly cropped image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        # Only perform random crop if random num is greater than 0.5\n",
    "        if np.random.uniform() > 0.5:\n",
    "            # Randomly choose width b.w. [self.min_crop_width, image width], analogously for height\n",
    "            desired_width = np.random.randint(low=self.min_crop_width, high=width + 1)\n",
    "            desired_height = np.random.randint(low=self.min_crop_height, high=height + 1)\n",
    "\n",
    "            # Randomly select top left point of our region\n",
    "            x = np.random.randint(low=0, high=width - desired_width + 1)\n",
    "            y = np.random.randint(low=0, high=height - desired_height + 1)\n",
    "\n",
    "            # Perform actual cropping on image and mask given above parameters\n",
    "            img = img.crop((x, y, x + desired_width - 1, y + desired_height - 1))\n",
    "            mask = mask.crop((x, y, x + desired_width - 1, y + desired_height - 1))\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"\n",
    "    A callable class to resize input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    desired_width : int\n",
    "        Desired width to resize the image to.\n",
    "    desired_height : int\n",
    "        Desired height to resize the image to.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Width and height for resize operation will be provided while object init.\n",
    "        Resized image and its mask will be returned as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, desired_width=1024, desired_height=1024):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing resizing operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            desired_width : int\n",
    "                Desired width to resize the image to.\n",
    "            desired_height : int\n",
    "                Desired height to resize the image to.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure parameter values are atlease 224 pixels\n",
    "        assert (desired_width >= 224 and desired_height >= 224)\n",
    "\n",
    "        self.desired_width = desired_width\n",
    "        self.desired_height = desired_height\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns resized image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of resized image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        # Performs actual resizing of image and mask\n",
    "        img = img.resize((self.desired_width, self.desired_height), resample=Image.NEAREST)\n",
    "        mask = mask.resize((self.desired_width, self.desired_height), resample=Image.NEAREST)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class ColorTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to apply color transform on just the input image, not mask image.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    brightness : float\n",
    "        Factor required to change the brightness of the image\n",
    "    contrast : float\n",
    "        Factor required to change the contrast of the image\n",
    "    saturation : float\n",
    "        Factor required to change the saturation of the image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        A number b.w. [max(0, 1 - self.brightness), 1 + self.brightness] generated for all three params.\n",
    "        Apply color transform on input image based on above generated parameters.\n",
    "        Returns transformed input image and its unchanged mask as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, brightness=0., contrast=0., saturation=0.):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing color transform operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            brightness : float\n",
    "                Factor required to change the brightness of the image\n",
    "            contrast : float\n",
    "                Factor required to change the contrast of the image\n",
    "            saturation : float\n",
    "                Factor required to change the saturation of the image\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure param values aren't negative\n",
    "        assert (brightness >= 0 and contrast >= 0 and saturation >= 0)\n",
    "\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns color transformed input image and its unchanged mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of color transformed input image and its unchanged mask.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get input and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "\n",
    "        # Generate a number b.w. [max(0, 1 - self.brightness), 1 + self.brightness] likewise for other two params\n",
    "        brightness_factor = np.random.uniform(max(0, 1 - self.brightness), 1 + self.brightness)\n",
    "        contrast_factor = np.random.uniform(max(0, 1 - self.contrast), 1 + self.contrast)\n",
    "        saturation_factor = np.random.uniform(max(0, 1 - self.saturation), 1 + self.saturation)\n",
    "\n",
    "        # For factor < 1 brightness of the image will be decreased, analogously for contrast and saturation.\n",
    "        # For factor = 1 original brightness will be retained, analogously for contrast and saturation.\n",
    "        # For factor > 1 brightness of the image will be increased, analogously for contrast and saturation.\n",
    "\n",
    "        # Apply actual color transform on input image based on above generated params.\n",
    "        img = ImageEnhance.Brightness(img).enhance(brightness_factor)\n",
    "        img = ImageEnhance.Contrast(img).enhance(contrast_factor)\n",
    "        img = ImageEnhance.Color(img).enhance(saturation_factor)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class FlipTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to flip input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Randomly flip (horizontally and vertically) both input image and mask image.\n",
    "        Resized image and its mask will be returned as a tuple.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns resized image and mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of resized image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get image and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "\n",
    "        # If randomly generated number (b.w. 0 and 1) is > 0.5 then flip (input image and mask image) horizontally else not.\n",
    "        if np.random.uniform() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # If randomly generated number (b.w. 0 and 1) is > 0.5 then flip (input image and mask image) vertically else not.\n",
    "        if np.random.uniform() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class AffineTransform(object):\n",
    "    \"\"\"\n",
    "    A callable class to apply affine transform on input image and its mask.\n",
    "    \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    degrees : float\n",
    "        Required to rotate image and mask to some random angle\n",
    "    translate : tuple of 2 vals\n",
    "        Required to shift (by dx in x-direction and by dy in y-direction) image and mask to some random parameters.\n",
    "    scale : tuple of 2 vals\n",
    "        Required to scale image and mask to some random param (like downscale or upscale to some degree).\n",
    "    shear : tuple of 4 vals\n",
    "        Required to shear image and mask to some random params.\n",
    "    resample : int\n",
    "        Resampling method for affine transform. Default is nearest neighbor strategy for segmentation task.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(sample):\n",
    "        Given tuple of input image and its corresponding mask image.\n",
    "        Draw random values given init args for rotation, translation, scaling and shearing.\n",
    "        Returns affine transformed image and its mask as a tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees=0., translate=(0, 0), scale=(1., 1.), shear=(0., 0., 0., 0.), resample=Image.BILINEAR):\n",
    "        \"\"\"\n",
    "        Accepts all the necessary attributes for performing affine transform.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            degrees : float\n",
    "                Required to rotate image and mask to some random angle\n",
    "            translate : tuple of 2 vals\n",
    "                Required to shift (by dx in x-direction and by dy in y-direction) image and mask to some random parameters.\n",
    "            scale : tuple of 2 vals\n",
    "                Required to scale image and mask to some random param (like downscale or upscale to some degree).\n",
    "            shear : tuple of 4 vals\n",
    "                Required to shear image and mask to some random params.\n",
    "            resample : int\n",
    "                Resampling method for affine transform. Default is nearest neighbor strategy for segmentation task.\n",
    "        \"\"\"\n",
    "\n",
    "        assert (0 <= degrees <= 180)\n",
    "        assert (all(0 <= val <= 1 for val in translate))\n",
    "        assert (all(val > 0 for val in scale))\n",
    "        self.degrees = degrees\n",
    "        self.translate = translate\n",
    "        self.scale = scale\n",
    "        self.shear = shear\n",
    "        self.resample = resample\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Returns affine tranformed image and its mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : tuple\n",
    "            Tuple of image and its corresponding mask image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of affine tranformed image and its mask\n",
    "        \"\"\"\n",
    "\n",
    "        # Get image and its mask from arg.\n",
    "        img, mask = sample[0], sample[1]\n",
    "        width, height = img.size\n",
    "\n",
    "        # Randomly generate angle b.w. [-self.degrees, self.degrees]\n",
    "        angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "\n",
    "        # Randomly generate dx and dy based on params to translate the image\n",
    "        max_dx = self.translate[0] * width\n",
    "        max_dy = self.translate[1] * height\n",
    "        translations = (np.round(np.random.uniform(-max_dx, max_dx)),\n",
    "                        np.round(np.random.uniform(-max_dy, max_dy)))\n",
    "\n",
    "        # Randomly generate new scale based on params\n",
    "        new_scale = np.random.uniform(self.scale[0], self.scale[1])\n",
    "\n",
    "        # Randomly generate shear ranges based on params\n",
    "        shear_ranges = [np.random.uniform(self.shear[0], self.shear[1]),\n",
    "                        np.random.uniform(self.shear[2], self.shear[3])]\n",
    "\n",
    "        # Apply affine transform based on above generated values on image and mask\n",
    "        img = transforms.functional.affine(img, angle=angle, translate=translations,\n",
    "                                           scale=new_scale, shear=shear_ranges, resample=self.resample)\n",
    "        mask = transforms.functional.affine(mask, angle=angle, translate=translations,\n",
    "                                            scale=new_scale, shear=shear_ranges, resample=self.resample)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Data augmentation pipeline for train images\n",
    "train_transform = transforms.Compose([RandomCrop(2048, 2048),\n",
    "                                      Resize(512, 512),\n",
    "                                      ColorTransform(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                                      FlipTransform(),\n",
    "                                      AffineTransform(degrees=180, translate=(.2, .2), scale=(0.75, 1.25),\n",
    "                                                      shear=[-30, 30, -30, 30], resample=Image.NEAREST)])\n",
    "\n",
    "# Transform for validation images\n",
    "val_transform = transforms.Compose([Resize(512, 512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get pixel distribution for each class in whole dataset\n",
    "def get_class_distribution(mask_path, nb_total_classes):\n",
    "    n = 0\n",
    "    class_info = {}\n",
    "    temp = np.zeros(nb_total_classes)\n",
    "    \n",
    "    for fn in os.listdir(mask_path):\n",
    "        mask = Image.open(os.path.join(mask_path, fn))\n",
    "        w, h = mask.size\n",
    "        arr = np.ravel(np.asarray(mask, dtype = np.int32))\n",
    "        temp += (np.bincount(arr, minlength = nb_total_classes)/(w*h))\n",
    "        n += 1\n",
    "            \n",
    "    temp /= n\n",
    "    percs = np.sort(temp)[::-1]\n",
    "    indices = np.argsort(temp)[::-1]\n",
    "    \n",
    "    return percs, indices\n",
    "\n",
    "# To select top-k classes (Excluding background class) also return their weights to pass it to cross entropy loss\n",
    "def select_top_k_classes(class_dist, class_indices, k=5):\n",
    "    assert(k <= class_dist.shape[0])\n",
    "    assert(len(class_dist) == len(class_indices))\n",
    "    class_info = {}\n",
    "    if np.any(class_indices[:k] == 0):\n",
    "        k += 1\n",
    "        zero_pos = np.where(class_indices == 0)[0]\n",
    "        keep_classes = class_indices[:k]\n",
    "        keep_classes_dist = class_dist[:k]\n",
    "        keep_classes_dist[zeros_pos] += np.sum(class_dist[k:])\n",
    "    else:\n",
    "        keep_classes = class_indices[:k]\n",
    "        keep_classes_dist = class_dist[:k]\n",
    "        keep_classes = np.append(keep_classes, 0)\n",
    "        keep_classes_dist = np.append(keep_classes_dist, np.sum(class_dist[k:]))\n",
    "    class_weights = 1/keep_classes_dist\n",
    "    class_info['class_weights'] = class_weights\n",
    "    class_info['keep_classes'] = keep_classes\n",
    "    return class_info\n",
    "\n",
    "class_dist, class_indices = get_class_distribution(mask_path, nb_total_classes = 23)\n",
    "keep_classes_info = select_top_k_classes(class_dist, class_indices, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, keep_classes_info):\n",
    "        # Path containing input images\n",
    "        self.img_path = img_path\n",
    "        \n",
    "        # Path containing mask images\n",
    "        self.mask_path = mask_path\n",
    "        \n",
    "        # Number of classes in output segmentation map\n",
    "        self.nb_classes = keep_classes_info['keep_classes'].shape[0]\n",
    "        \n",
    "        # Store filenames of all the images in a list\n",
    "        self.filenames = []\n",
    "        self.data_len = 0\n",
    "        for fn in os.listdir(img_path):\n",
    "            self.filenames.append(fn.split('.')[0])\n",
    "            self.data_len += 1\n",
    "           \n",
    "        # Generate a look up table to remove unwanted class from mask images \n",
    "        self.lut = np.zeros(256,dtype=np.uint8)\n",
    "        \n",
    "        # Rearrange class weights according to new class_ids\n",
    "        self.class_weights = np.zeros(self.nb_classes)\n",
    "        new_id = 1\n",
    "        for class_id, class_weight in zip(keep_classes_info['keep_classes'], keep_classes_info['class_weights']):\n",
    "            if class_id:\n",
    "                self.lut[class_id] = new_id\n",
    "                self.class_weights[new_id] = class_weight\n",
    "                new_id += 1\n",
    "            else:\n",
    "                self.class_weights[class_id] = class_weight\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Returns dataset size\n",
    "        '''\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "         # Read image (mask image too) on given idx from list\n",
    "        img = Image.open(os.path.join(img_path, self.filenames[idx]) + '.jpg')\n",
    "        mask = Image.open(os.path.join(mask_path, self.filenames[idx]) + '.png')\n",
    "        \n",
    "        # Replace unwanted class in mask image from LUT\n",
    "        np_mask_img = np.array(mask)\n",
    "        mask = Image.fromarray(self.lut[np_mask_img])\n",
    "        \n",
    "        return img, mask\n",
    "    \n",
    "# Map inputs to desired state by passing them through augmentation pipeline\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, nb_classes, transform = None, mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]):\n",
    "        self.dataset = dataset\n",
    "        self.nb_classes = nb_classes\n",
    "        self.transform = transform\n",
    "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            img, mask = self.transform(self.dataset[idx])\n",
    "            \n",
    "        # Plot augmented image and its corresponding mask\n",
    "#         plt.imshow(np.array(img))\n",
    "#         plt.show()\n",
    "#         plt.imshow(np.array(mask))\n",
    "#         plt.show()\n",
    "\n",
    "        # Convert image to torch tensor\n",
    "        img = self.toTensor(img)\n",
    "        mask = torch.from_numpy(np.array(mask))\n",
    "        \n",
    "        # Mean normalization on image and one hot encode the mask image\n",
    "        img = self.normalize(img)\n",
    "        mask = F.one_hot(mask.to(torch.int64), num_classes = self.nb_classes).permute(2, 0, 1).to(torch.float32)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Creating dataset class\n",
    "data = CustomDataset(img_path, mask_path, keep_classes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random indices for train, val and test set\n",
    "val_split = .1\n",
    "\n",
    "dataset_size = len(data)\n",
    "np.random.seed(4)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "split = int(np.floor(val_split * dataset_size))\n",
    "\n",
    "# Divide the data into train, val and test dataset given above indices\n",
    "train_dataset = TransformDataset(Subset(data, indices[(2 * split):]), data.nb_classes, train_transform)\n",
    "val_dataset = TransformDataset(Subset(data, indices[split: (2 * split)]), data.nb_classes, val_transform)\n",
    "test_dataset = TransformDataset(Subset(data, indices[:split]), data.nb_classes, val_transform)\n",
    "\n",
    "# Create data loader for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, num_workers = 2, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 40, num_workers = 2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 40, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss criterion for image segmentation problem\n",
    "# Combination of dice loss and cross entropy loss is excellent choice\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight = None):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        if weight is not None:\n",
    "            self.weight = torch.cuda.FloatTensor(weight)\n",
    "        else:\n",
    "            self.weight = None\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        # Apply softmax to get prob. distribution for each pixel\n",
    "        probs = F.softmax(inputs, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gt = torch.argmax(targets, 1)\n",
    "        \n",
    "        BCE = F.cross_entropy(inputs, gt, weight = self.weight, reduction='mean')\n",
    "        \n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (probs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(probs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "# Dice score used as an accuracy metric\n",
    "def calculateDiceScore(preds, targets, smooth = 1):\n",
    "    preds = F.softmax(preds, 1)       \n",
    "        \n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (preds * targets).sum()                            \n",
    "    return (2.*intersection + smooth)/(preds.sum() + targets.sum() + smooth)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dfe5db79454e3d9d85d6b555060771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n"
     ]
    }
   ],
   "source": [
    "# Loading model from torchvision\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=False, num_classes=21, aux_loss=None)\n",
    "\n",
    "# Change last layer for our requirement (we have k+1 classes total including background)\n",
    "model.classifier[4] = nn.Conv2d(256, data.nb_classes, (1, 1), (1, 1))\n",
    "model.aux_classifier[4] = nn.Conv2d(256, data.nb_classes, (1, 1), (1, 1))\n",
    "\n",
    "# Freeze some of the initial layers to finetune the model\n",
    "idx = 0\n",
    "for name, param in model.named_parameters():\n",
    "    # print(idx, name)\n",
    "    if idx >= 165:\n",
    "        break\n",
    "    param.requires_grad = False\n",
    "    idx += 1\n",
    "    \n",
    "# Defining device to cuda if GPU is avaliable else to cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining loss criterion\n",
    "criterion = DiceBCELoss(weight = data.class_weights)\n",
    "\n",
    "# Defining optimizer to update model params (Only some of the last layers), Adam's a good default\n",
    "optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "\n",
    "# Learning rate scheduler to update lr when loss stops improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTrain Loss: 1.5789325475692748\tVal Loss: 0.9795246720314026\n",
      "Epoch 2:\tTrain Loss: 0.9426460683345794\tVal Loss: 0.6723008155822754\n",
      "Epoch 3:\tTrain Loss: 0.8570941805839538\tVal Loss: 0.6240894794464111\n",
      "Epoch 4:\tTrain Loss: 0.8086203053593636\tVal Loss: 0.8219184279441833\n",
      "Epoch 5:\tTrain Loss: 0.7614537358283997\tVal Loss: 0.7400764226913452\n",
      "Epoch 6:\tTrain Loss: 0.7334292367100715\tVal Loss: 0.8786985874176025\n",
      "Epoch 7:\tTrain Loss: 0.6875955708324909\tVal Loss: 0.5164713859558105\n",
      "Epoch 8:\tTrain Loss: 0.6216550342738628\tVal Loss: 0.5003544688224792\n",
      "Epoch 9:\tTrain Loss: 0.6054667018353939\tVal Loss: 0.4883110523223877\n",
      "Epoch 10:\tTrain Loss: 0.5991924457252026\tVal Loss: 0.47872114181518555\n",
      "Epoch 11:\tTrain Loss: 0.5734261631965637\tVal Loss: 0.4770793616771698\n",
      "Epoch 12:\tTrain Loss: 0.5523261554539204\tVal Loss: 0.4732629954814911\n",
      "Epoch 13:\tTrain Loss: 0.5539035253226757\tVal Loss: 0.46066129207611084\n",
      "Epoch 14:\tTrain Loss: 0.5280256122350693\tVal Loss: 0.4582401216030121\n",
      "Epoch 15:\tTrain Loss: 0.5706786625087261\tVal Loss: 0.46187740564346313\n",
      "Epoch 16:\tTrain Loss: 0.5235912203788757\tVal Loss: 0.44945186376571655\n",
      "Epoch 17:\tTrain Loss: 0.5250880300998688\tVal Loss: 0.4565120041370392\n",
      "Epoch 18:\tTrain Loss: 0.5326054275035859\tVal Loss: 0.4567473828792572\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58066664aad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Empty cuda cache to clear useless data from VRAM for better utilization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path at which to store model and required configs\n",
    "model_save_path = 'aerial_imagery_seg_model.pt'\n",
    "\n",
    "# Total epochs to train for\n",
    "epochs = 50\n",
    "\n",
    "# Keep track of min val loss\n",
    "min_val_loss = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0.\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # A pass through train dataset\n",
    "    for imgs, masks in train_loader:\n",
    "        # Move batch data to device\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass the batch and get predictions\n",
    "        preds = model(imgs)['out']\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(preds, masks)\n",
    "        \n",
    "        # Add to calculate loss for whole dataset\n",
    "        train_loss += (loss.item() * imgs.size(0))\n",
    "        \n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make weight updates\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Empty cuda cache to clear useless data from VRAM for better utilization\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Switch model to inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Check model performance on val set\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            preds = model(imgs)['out']\n",
    "\n",
    "            loss = criterion(preds, masks)\n",
    "            val_loss += (loss.item() * imgs.size(0))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    # Calculate avg train and avg val loss\n",
    "    train_loss /= len(train_dataset)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    # Decrease lr depending on val_loss & given params (object init)\n",
    "    scheduler.step(val_loss)\n",
    "        \n",
    "    # If loss is decreasing then store model in file else not\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model, model_save_path)\n",
    "        \n",
    "    print('Epoch {}:\\tTrain Loss: {}\\tVal Loss: {}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_classifier): FCNHead(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model from saved file\n",
    "model = torch.load(model_save_path)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Switch model to inference mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8391628265380859\n"
     ]
    }
   ],
   "source": [
    "# Calculate dice score on test set, to get test accuracy\n",
    "with torch.no_grad():\n",
    "    test_acc = 0       \n",
    "    for imgs, masks in test_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        preds = model(imgs)['out']\n",
    "\n",
    "        dice_score = calculateDiceScore(preds, masks).item()\n",
    "        test_acc += (dice_score * imgs.size(0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Test Acc: {}'.format(test_acc / len(test_dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
