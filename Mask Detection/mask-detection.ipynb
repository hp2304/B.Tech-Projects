{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Deep NN as classifier\nclass DNN(nn.Module):\n    def __init__(self, use_pretrained = True, nb_outs = 1):\n        super(DNN, self).__init__()\n        \n        # Mobilenet model to finetune for our problem\n        self.net = models.mobilenet_v2(pretrained = use_pretrained, progress=True)\n        \n        # Altering last layer to our need\n        self.net.classifier = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(1280, 1, bias=False))\n        \n    def forward(self, x):\n        x = self.net(x)\n        return x\n  \n# Model init\nmodel = DNN()\n\n# Move model to device\nmodel = model.to(device)\n\n# Freeze inititial layers of mobilenet\ni = 0\nfor name, param in model.named_parameters():\n    print(i, name)\n    if i < 135:\n        param.requires_grad = False\n    i+=1","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/checkpoints/mobilenet_v2-b0353104.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f872fb9ea5439d99303b2136375bee"}},"metadata":{}},{"output_type":"stream","text":"\n0 net.features.0.0.weight\n1 net.features.0.1.weight\n2 net.features.0.1.bias\n3 net.features.1.conv.0.0.weight\n4 net.features.1.conv.0.1.weight\n5 net.features.1.conv.0.1.bias\n6 net.features.1.conv.1.weight\n7 net.features.1.conv.2.weight\n8 net.features.1.conv.2.bias\n9 net.features.2.conv.0.0.weight\n10 net.features.2.conv.0.1.weight\n11 net.features.2.conv.0.1.bias\n12 net.features.2.conv.1.0.weight\n13 net.features.2.conv.1.1.weight\n14 net.features.2.conv.1.1.bias\n15 net.features.2.conv.2.weight\n16 net.features.2.conv.3.weight\n17 net.features.2.conv.3.bias\n18 net.features.3.conv.0.0.weight\n19 net.features.3.conv.0.1.weight\n20 net.features.3.conv.0.1.bias\n21 net.features.3.conv.1.0.weight\n22 net.features.3.conv.1.1.weight\n23 net.features.3.conv.1.1.bias\n24 net.features.3.conv.2.weight\n25 net.features.3.conv.3.weight\n26 net.features.3.conv.3.bias\n27 net.features.4.conv.0.0.weight\n28 net.features.4.conv.0.1.weight\n29 net.features.4.conv.0.1.bias\n30 net.features.4.conv.1.0.weight\n31 net.features.4.conv.1.1.weight\n32 net.features.4.conv.1.1.bias\n33 net.features.4.conv.2.weight\n34 net.features.4.conv.3.weight\n35 net.features.4.conv.3.bias\n36 net.features.5.conv.0.0.weight\n37 net.features.5.conv.0.1.weight\n38 net.features.5.conv.0.1.bias\n39 net.features.5.conv.1.0.weight\n40 net.features.5.conv.1.1.weight\n41 net.features.5.conv.1.1.bias\n42 net.features.5.conv.2.weight\n43 net.features.5.conv.3.weight\n44 net.features.5.conv.3.bias\n45 net.features.6.conv.0.0.weight\n46 net.features.6.conv.0.1.weight\n47 net.features.6.conv.0.1.bias\n48 net.features.6.conv.1.0.weight\n49 net.features.6.conv.1.1.weight\n50 net.features.6.conv.1.1.bias\n51 net.features.6.conv.2.weight\n52 net.features.6.conv.3.weight\n53 net.features.6.conv.3.bias\n54 net.features.7.conv.0.0.weight\n55 net.features.7.conv.0.1.weight\n56 net.features.7.conv.0.1.bias\n57 net.features.7.conv.1.0.weight\n58 net.features.7.conv.1.1.weight\n59 net.features.7.conv.1.1.bias\n60 net.features.7.conv.2.weight\n61 net.features.7.conv.3.weight\n62 net.features.7.conv.3.bias\n63 net.features.8.conv.0.0.weight\n64 net.features.8.conv.0.1.weight\n65 net.features.8.conv.0.1.bias\n66 net.features.8.conv.1.0.weight\n67 net.features.8.conv.1.1.weight\n68 net.features.8.conv.1.1.bias\n69 net.features.8.conv.2.weight\n70 net.features.8.conv.3.weight\n71 net.features.8.conv.3.bias\n72 net.features.9.conv.0.0.weight\n73 net.features.9.conv.0.1.weight\n74 net.features.9.conv.0.1.bias\n75 net.features.9.conv.1.0.weight\n76 net.features.9.conv.1.1.weight\n77 net.features.9.conv.1.1.bias\n78 net.features.9.conv.2.weight\n79 net.features.9.conv.3.weight\n80 net.features.9.conv.3.bias\n81 net.features.10.conv.0.0.weight\n82 net.features.10.conv.0.1.weight\n83 net.features.10.conv.0.1.bias\n84 net.features.10.conv.1.0.weight\n85 net.features.10.conv.1.1.weight\n86 net.features.10.conv.1.1.bias\n87 net.features.10.conv.2.weight\n88 net.features.10.conv.3.weight\n89 net.features.10.conv.3.bias\n90 net.features.11.conv.0.0.weight\n91 net.features.11.conv.0.1.weight\n92 net.features.11.conv.0.1.bias\n93 net.features.11.conv.1.0.weight\n94 net.features.11.conv.1.1.weight\n95 net.features.11.conv.1.1.bias\n96 net.features.11.conv.2.weight\n97 net.features.11.conv.3.weight\n98 net.features.11.conv.3.bias\n99 net.features.12.conv.0.0.weight\n100 net.features.12.conv.0.1.weight\n101 net.features.12.conv.0.1.bias\n102 net.features.12.conv.1.0.weight\n103 net.features.12.conv.1.1.weight\n104 net.features.12.conv.1.1.bias\n105 net.features.12.conv.2.weight\n106 net.features.12.conv.3.weight\n107 net.features.12.conv.3.bias\n108 net.features.13.conv.0.0.weight\n109 net.features.13.conv.0.1.weight\n110 net.features.13.conv.0.1.bias\n111 net.features.13.conv.1.0.weight\n112 net.features.13.conv.1.1.weight\n113 net.features.13.conv.1.1.bias\n114 net.features.13.conv.2.weight\n115 net.features.13.conv.3.weight\n116 net.features.13.conv.3.bias\n117 net.features.14.conv.0.0.weight\n118 net.features.14.conv.0.1.weight\n119 net.features.14.conv.0.1.bias\n120 net.features.14.conv.1.0.weight\n121 net.features.14.conv.1.1.weight\n122 net.features.14.conv.1.1.bias\n123 net.features.14.conv.2.weight\n124 net.features.14.conv.3.weight\n125 net.features.14.conv.3.bias\n126 net.features.15.conv.0.0.weight\n127 net.features.15.conv.0.1.weight\n128 net.features.15.conv.0.1.bias\n129 net.features.15.conv.1.0.weight\n130 net.features.15.conv.1.1.weight\n131 net.features.15.conv.1.1.bias\n132 net.features.15.conv.2.weight\n133 net.features.15.conv.3.weight\n134 net.features.15.conv.3.bias\n135 net.features.16.conv.0.0.weight\n136 net.features.16.conv.0.1.weight\n137 net.features.16.conv.0.1.bias\n138 net.features.16.conv.1.0.weight\n139 net.features.16.conv.1.1.weight\n140 net.features.16.conv.1.1.bias\n141 net.features.16.conv.2.weight\n142 net.features.16.conv.3.weight\n143 net.features.16.conv.3.bias\n144 net.features.17.conv.0.0.weight\n145 net.features.17.conv.0.1.weight\n146 net.features.17.conv.0.1.bias\n147 net.features.17.conv.1.0.weight\n148 net.features.17.conv.1.1.weight\n149 net.features.17.conv.1.1.bias\n150 net.features.17.conv.2.weight\n151 net.features.17.conv.3.weight\n152 net.features.17.conv.3.bias\n153 net.features.18.0.weight\n154 net.features.18.1.weight\n155 net.features.18.1.bias\n156 net.classifier.1.weight\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_nums = [0.485, 0.456, 0.406]\nstd_nums = [0.229, 0.224, 0.225]\n\n# Visulize batch images\ndef visulize_batch(x):\n    for i in range(x.shape[0]):\n        img = x[i, :, :, :].numpy().transpose((1,2,0))\n        img = (img*std_nums) + mean_nums\n        img = np.clip(img, 0, 1)\n        plt.imshow(img)\n        plt.show()    \n\n# Data transform pipeline for training set\ntrain_transform = transforms.Compose([\n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomAffine(degrees = 30, translate=(.2, .2), scale=(0.8, 1.2), shear=[-10, 10, -10, 10]),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_nums, std_nums)\n])\n\n# Data transform pipeline for validation set\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_nums, std_nums)\n])\n\n# Dataset init for train and val\ntrain_dataset = datasets.ImageFolder(\"../input/face-mask-12k-images-dataset/Face Mask Dataset/Train\", transform=train_transform)\nval_dataset = datasets.ImageFolder(\"../input/face-mask-12k-images-dataset/Face Mask Dataset/Validation\", transform=val_transform)\n\n# Dataloader to batch input data\ntrain_loader = DataLoader(train_dataset, batch_size = 64, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size = 64, shuffle=True, num_workers=4)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for x, target in train_loader:\n#     print(target)\n#     visulize_batch(x)\n#     break","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binary cross entropy loss\ncriterion = nn.BCEWithLogitsLoss()\n\n# Adam optimizer to update model params (Only some of last layers)\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n\n# LR scheduler to decrease lr if val_loss stops improving\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of epochs to train for\nnum_epochs = 10\n\n# Keep track of min val loss\nmin_val_loss = np.inf\n\n# Training loop\nfor i in range(num_epochs):\n    train_loss = 0.\n    val_loss = 0.\n    \n    # Switch model to training mode\n    model.train()\n    \n    # Looping through train set\n    for inputs, labels in train_loader:\n        # Move batch data to device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Clear out previous gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        preds = model(inputs).squeeze(1)\n        labels = labels.type_as(preds)\n        \n        # Compute loss given labels and predictions\n        loss = criterion(preds, labels)\n        train_loss += (loss.item()*inputs.size(0))\n        \n        # Backpropagate to calculate gradients w.r.t. loss\n        loss.backward()\n        \n        # Make weight updates\n        optimizer.step()\n\n    # Switch model to evaluation mode\n    model.eval()\n    \n    # Never compute gradients while evaluating\n    with torch.no_grad():\n        \n        # Looping through val set to find val_loss\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            preds = model(inputs).squeeze(1)\n            labels = labels.type_as(preds)\n\n            loss = criterion(preds, labels)\n\n            val_loss += (loss.item()*inputs.size(0))\n      \n    # Calculate avg. train and val loss\n    train_loss /= len(train_dataset)\n    val_loss /= len(val_dataset)\n    \n    # If val_loss is minimum than seen before save the model\n    if val_loss < min_val_loss:\n        min_val_loss = val_loss\n        torch.save(model.state_dict(), 'Mobilenet_v2_Mask_Detection.pt')\n    \n    # Change lr if val_loss not improving given init params\n    scheduler.step(val_loss)\n        \n    print('Epoch {}:\\tTrain Loss: {:.6f}\\tVal Loss: {:.6f}'.format(i+1, train_loss, val_loss))","execution_count":6,"outputs":[{"output_type":"stream","text":"Epoch 1:\tTrain Loss: 0.035990\tVal Loss: 0.007323\nEpoch 2:\tTrain Loss: 0.011301\tVal Loss: 0.001499\nEpoch 3:\tTrain Loss: 0.008590\tVal Loss: 0.000372\nEpoch 4:\tTrain Loss: 0.007054\tVal Loss: 0.003165\nEpoch 5:\tTrain Loss: 0.011228\tVal Loss: 0.000410\nEpoch 6:\tTrain Loss: 0.004974\tVal Loss: 0.000011\nEpoch 7:\tTrain Loss: 0.008755\tVal Loss: 0.000918\nEpoch 8:\tTrain Loss: 0.007316\tVal Loss: 0.000025\nEpoch 9:\tTrain Loss: 0.004920\tVal Loss: 0.000080\nEpoch 10:\tTrain Loss: 0.003205\tVal Loss: 0.000017\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}