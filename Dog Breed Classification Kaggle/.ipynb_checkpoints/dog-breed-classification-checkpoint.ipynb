{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73475a81-5f68-48ee-97f8-e97b8e811d76",
    "_uuid": "423dc9aa-9548-448c-b857-25fcce97f5ab"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Importing pytorch\n",
    "from torchvision import models, transforms # To get pretrained deep NNs and various image transforms\n",
    "from torch.utils.data import Dataset, DataLoader # To create custom dataset classes and to batch data respectively\n",
    "from torch import nn, optim # NN module to customize network and to get loss functions, optim to get various optimizers\n",
    "from PIL import Image # to load images\n",
    "import matplotlib.pyplot as plt # to plot images\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class DogBreedDataset(Dataset):\n",
    "    def __init__(self, training_dir_path, train_df, label2id, transform):\n",
    "        self.training_dir_path = training_dir_path\n",
    "        self.df = train_df\n",
    "        self.label2id = label2id\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return size of the dataset\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read image using PIL\n",
    "        image = Image.open(os.path.join(self.training_dir_path, self.df.iloc[idx, 0]) + '.jpg')\n",
    "        \n",
    "        # Extract image's true label from dataframe\n",
    "        label = label2id[self.df.iloc[idx, 1]]\n",
    "        \n",
    "        # Transform image if specified (eg. Resize, Crop, flip, etc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any DNN with pre-trained weights and modify classifier to output N numbers\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 120, bias = True)\n",
    "model = models.densenet161(pretrained=True)\n",
    "model.classifier = nn.Linear(2208, 120)\n",
    "\n",
    "# for i, layer in enumerate(model.parameters()):\n",
    "#     print(i, layer.shape)\n",
    "#     if i < 9:\n",
    "#         layer.requires_grad_ = False\n",
    "# print(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file containing image file names and it's corresponding breed name. Also shuffle after reading\n",
    "labels = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv').sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Get all unique breed names\n",
    "unique_labels = labels.breed.unique()\n",
    "unique_labels.sort()\n",
    "\n",
    "# Make dict to map breed name to a index b.w 0 to N-1\n",
    "label2id = {}\n",
    "for idx, name in enumerate(unique_labels):\n",
    "    label2id[name] = idx\n",
    "\n",
    "training_dir_path = '/kaggle/input/dog-breed-identification/train'\n",
    "labels_path = '/kaggle/input/dog-breed-identification/labels.csv'\n",
    "    \n",
    "def get_train_val_loader(labels, label2id, training_dir_path, labels_path, train_frac = 1, batch_size = 32, nb_workers = 4):\n",
    "    # Total number of training examples\n",
    "    nb_exmpls = len(labels.index)\n",
    "    \n",
    "    # Split original dataframe to train and validation df\n",
    "    split = int(np.floor(train_frac * nb_exmpls))\n",
    "    train_df = labels.iloc[:split, :]\n",
    "    val_df = labels.iloc[split:, :]\n",
    "\n",
    "    # Transform for train set\n",
    "    train_transform = transforms.Compose([transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.4),\n",
    "                                        transforms.Resize((224, 224)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomAffine(degrees = 30, scale = (.75, 1.25)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        #transforms.Normalize([0, 0, 0], [1, 1, 1])])\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    # Create dataset class for train set\n",
    "    train_dataset = DogBreedDataset(training_dir_path, train_df, label2id, train_transform)\n",
    "    \n",
    "    # Create data loader to batch data on the fly to feed the model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=nb_workers, shuffle=True)\n",
    "    \n",
    "    if train_frac == 1:\n",
    "        return (train_dataset, train_loader)\n",
    "    else:\n",
    "        # Transform for validation set\n",
    "        val_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            #transforms.Normalize([0, 0, 0], [1, 1, 1])])\n",
    "                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "        # Create dataset class for validation set\n",
    "        val_dataset = DogBreedDataset(training_dir_path, val_df, label2id, val_transform)\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=nb_workers, shuffle=True)\n",
    "        \n",
    "        return (train_dataset, train_loader, val_dataset, val_loader)\n",
    "    \n",
    "batch_size = 24\n",
    "train_dataset, train_loader, val_dataset, val_loader = get_train_val_loader(labels, label2id, training_dir_path, labels_path, batch_size = batch_size, train_frac = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulize images by reading a batch from loaders\n",
    "# for images, targets in train_loader:\n",
    "#     for i in range(batch_size):\n",
    "#         img = images[i, :, :, :].numpy().transpose((1, 2, 0))\n",
    "#         mean = np.array([[[.485, 0.456, 0.406]]])\n",
    "#         std = np.array([[[.229, 0.224, 0.225]]])\n",
    "#         img = img*std + mean\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(unique_labels[int(targets[i].item())])\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss for multiclass classification and Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of epochs to train for\n",
    "n_epochs = 100\n",
    "\n",
    "# Init max val acc to 0\n",
    "val_acc_max = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through training set by processing batches\n",
    "    for data, target in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Make predictions and calculate correct predictions\n",
    "        _, preds = torch.max(output, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += torch.sum(preds == target).cpu().data.item()\n",
    "        \n",
    "        # Calculate loss and update the weights\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # Calculate avg loss over whole training set\n",
    "    train_loss = train_loss/len(train_dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(epoch+1, train_loss, correct/total))\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Do forward pass through validation data and calculate val. loss\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, preds = torch.max(output, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += torch.sum(preds == target).cpu().data.item()\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    val_loss = val_loss/len(val_dataset)\n",
    "    \n",
    "    print('Val Loss: {:.6f} \\t Val Accuracy: {:.6f}'.format(val_loss, correct/total))\n",
    "    \n",
    "    # save model if validation acc has increased\n",
    "    if (correct/total) > val_acc_max:\n",
    "        print('Validation Acc increased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_acc_max, correct/total))\n",
    "        torch.save(model.state_dict(), '/kaggle/working/dog_breed_model.pt')\n",
    "        val_acc_max = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test images' filenames into a dataframe\n",
    "test_dir_path = '/kaggle/input/dog-breed-identification/test'\n",
    "filenames = []\n",
    "for filename in os.listdir(test_dir_path):\n",
    "    filenames.append(filename.split('.')[0])\n",
    "out = pd.DataFrame(filenames, columns = ['id'])\n",
    "out = out.sort_values(by = 'id')\n",
    "out = out.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for test images\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir_path, test_df, transform):\n",
    "        self.test_dir_path = test_dir_path\n",
    "        self.df = test_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return size of the dataset\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read image using PIL\n",
    "        image = Image.open(os.path.join(self.test_dir_path, self.df.iloc[idx, 0]) + '.jpg')\n",
    "        \n",
    "        # Transform image if specified (eg. Resize, Crop, flip, etc)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "batch_size = 128\n",
    "test_dataset = TestDataset(test_dir_path, out, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model and run forward pass on test images\n",
    "model_path = '/kaggle/working/dog_breed_model.pt'\n",
    "\n",
    "preds = np.zeros((batch_size, len(unique_labels)), dtype=np.float32)\n",
    "\n",
    "saved_model = models.resnet18(pretrained=False)\n",
    "saved_model.fc = nn.Linear(512, 120, bias=True)\n",
    "saved_model.load_state_dict(torch.load(model_path))\n",
    "saved_model = saved_model.to(device)\n",
    "saved_model.eval()\n",
    "\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    output = saved_model(data)\n",
    "    preds = np.vstack((preds, output.detach().cpu().numpy()))\n",
    "preds = preds[batch_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probability distribution (softmax)\n",
    "preds = np.exp(preds)\n",
    "accm = np.sum(preds, axis=1)\n",
    "accm = accm[:, np.newaxis]\n",
    "preds /= accm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to csv file as shown in sample submission file\n",
    "preds_df = pd.DataFrame(data=preds, columns=unique_labels)\n",
    "submission = pd.concat([out, preds_df], axis = 1, ignore_index=True)\n",
    "submission.to_csv (r'/kaggle/working/submission.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
